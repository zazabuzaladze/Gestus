{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689cd0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python --version\n",
    "#nötig ist version 3.8.17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa77715",
   "metadata": {},
   "source": [
    "### Bibliotheken, die ich benutze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "527ba62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp    #model erstellen\n",
    "import numpy as np        #mit arrays arbeiten\n",
    "import os                 #mit dem Dateisystem interagieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e3cb1f",
   "metadata": {},
   "source": [
    "### Load my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e153a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split         #datensatz teilen für testen und trainieren\n",
    "from tensorflow.keras.utils import to_categorical          \n",
    "\n",
    "# Liste aller Gesten abrufen\n",
    "# https://www.geeksforgeeks.org/python-list-files-in-a-directory/\n",
    "\n",
    "path = os.path.join('datensatz1')\n",
    "gesten_list = os.listdir(path)\n",
    "\n",
    "#np array ist später nötig für trainieren\n",
    "gesten = np.array(gesten_list)\n",
    "\n",
    "\n",
    "#liste in der dictionary umwandeln und numarieren\n",
    "label_map = {}\n",
    "n = 0\n",
    "for label in gesten_list:\n",
    "    label_map[gesten_list[n]] = n\n",
    "    n += 1\n",
    "\n",
    "#wir müssen hier anpassen mithilfe von unsere datensatz, wie viele videos wir pro geste aufgenommen haben und wie viele frame hat jede video\n",
    "num_videos = 30\n",
    "vid_length = 20\n",
    "\n",
    "#mein datensatz verzeichnis\n",
    "DATA_PATH = os.path.join('datensatz1')\n",
    "\n",
    "videos = []\n",
    "labels = []\n",
    "\n",
    "#alle videos k-points in eine array zusammenfassen\n",
    "for geste in gesten:\n",
    "    for vid in range(num_videos):\n",
    "        window = []\n",
    "        for frame_num in range(vid_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, geste, str(vid), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        videos.append(window)\n",
    "        labels.append(label_map[geste])\n",
    "\n",
    "#umwandeln in numpy array\n",
    "#kp = k-points\n",
    "kp = np.array(videos)\n",
    "\n",
    "#gn = geste_name\n",
    "gn = to_categorical(labels).astype(int)\n",
    "\n",
    "#datensatz teilen in test- und trainierenteil\n",
    "kp_train, kp_test, gn_train, gn_test = train_test_split(kp, gn, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc42fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "#Die sequentielle ist nützlicher als die funktionale, da wir eine Dateneingabe und eine Datenausgabe haben.\n",
    "#Es leitet die Daten weiter und fließt in sequentieller Reihenfolge von oben nach unten, bis die Daten am Ende des Modells ankommen.\n",
    "#https://www.educba.com/keras-sequential/\n",
    "#https://www.youtube.com/watch?v=8uC-WT1LYnU&ab_channel=Simplilearn\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "#LSTM Layer: ermöglicht uns die Erkennung von Aktionen\n",
    "#Keras Dense-Schicht ist die Schicht, die alle Neuronen enthält, die in sich selbst tief verbunden sind. Das bedeutet, dass jedes Neuron in der dichten Schicht die Eingabe von allen anderen Neuronen der vorherigen Schicht erhält. Wir können so viele dichte Schichten wie nötig hinzufügen. Sie ist eine der am häufigsten verwendeten Schichten.\n",
    "#https://www.educba.com/keras-dense/\n",
    "#https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(20, 258)))\n",
    "#64 LSTM Units     \n",
    "#return sequences = True: weil die nächste Schicht dies braucht\n",
    "#activation = relu: Funktion, die die Eingabe direkt ausgibt, wenn sie positiv ist, andernfalls gibt sie Null aus.\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "#return sequences = False: weil die nächste Schicht ist Dense layer/schicht\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "#letzte Schicht\n",
    "model.add(Dense(gesten.shape[0], activation='softmax'))\n",
    "#Softmax ist eine Aktivierungsfunktion, die Zahlen/Logits in Wahrscheinlichkeiten umwandelt. \n",
    "#Die Ausgabe einer Softmax-Funktion ist ein Vektor (z. B. v) mit Wahrscheinlichkeiten für jedes mögliche Ergebnis. \n",
    "#Die Wahrscheinlichkeiten im Vektor v summieren sich für alle möglichen Ergebnisse oder Klassen zu eins.\n",
    "#https://towardsdatascience.com/softmax-activation-function-how-it-actually-works-d292d335bd78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fea75e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model/gewichte\n",
    "\n",
    "model.load_weights(\"gesten1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a2350f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20, 64)            82688     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 20, 128)           98816     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 237350 (927.15 KB)\n",
      "Trainable params: 237350 (927.15 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Testen, ob das Modell da ist\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c868e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#siehe Genauigkeit der Modelle\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "yhat = model.predict(kp_train)\n",
    "ytrue = np.argmax(gn_train, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()\n",
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a804b89a",
   "metadata": {},
   "source": [
    "### Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd62cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv      #zugriff auf kamera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7712579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelle erstellen\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "#holistic kombiert hände, körper und gesicht erkennung\n",
    "#https://developers.google.com/mediapipe/solutions/vision/holistic_landmarker\n",
    "#https://github.com/google/mediapipe/blob/master/docs/solutions/holistic.md\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "#farben umwandlung + model hinzufügen\n",
    "def conversion(frame, model):\n",
    "    frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "    #hier umwandeln wir frame in rgb weil mediapipe braucht rgb format\n",
    "    \n",
    "    frame.flags.writeable = False\n",
    "    results = model.process(frame)\n",
    "    #hier hinzufügen wir mediapipe model/\"eine Vorhersage treffen\"\n",
    "    \n",
    "    frame.flags.writeable = True\n",
    "    frame = cv.cvtColor(frame, cv.COLOR_RGB2BGR)\n",
    "    #hier zurückkehren wir in BGR damit cv2 es zeigen kann\n",
    "    \n",
    "    return frame, results\n",
    "\n",
    "#orientierungspunkte zeigen lassen an cv\n",
    "def show_kpoints(main_frame, results):\n",
    "    #wir werden gesichtspunkte nicht gebrauchen bei gebärdensprache\n",
    "    #mp_drawing.draw_landmarks(main_frame, results.face_landmarks, mp_holistic.FACE_CONNECTIONS)\n",
    "    \n",
    "    #                         image       orientierungspunkte     verbindungskarte\n",
    "    mp_drawing.draw_landmarks(main_frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(main_frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(main_frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95d95435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-points extrahieren\n",
    "\n",
    "#wir verwenden die \"try, except\"-Methode, falls nicht alle Schlüsselpunkte in der Kamera angezeigt werden, so dass es nicht zu einem Fehler kommt\n",
    "def extract_keypoints(results):\n",
    "    try:\n",
    "        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten()\n",
    "    except:\n",
    "        pose = np.zeros(33*4)\n",
    "\n",
    "    try:\n",
    "        linke_hand = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten()\n",
    "    except:\n",
    "        linke_hand = np.zeros(21*3)\n",
    "\n",
    "    try:\n",
    "        rechte_hand = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten()\n",
    "    except:\n",
    "        rechte_hand = np.zeros(21*3)\n",
    "    \n",
    "    return np.concatenate([pose, linke_hand, rechte_hand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59286856",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = []\n",
    "satz = []\n",
    "threshold = 0.6\n",
    "\n",
    "\n",
    "capture = cv.VideoCapture(1)\n",
    "# 1 = mein computer webcam\n",
    "# 0 = mein externe webcam/handy\n",
    "\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while capture.isOpened():\n",
    "        isTrue, frame = capture.read()\n",
    "\n",
    "        flipped_frame = cv.flip(frame, 1)   #bild spiegeln\n",
    "\n",
    "        main_frame, results = conversion(flipped_frame, holistic)\n",
    "\n",
    "        #orientierungspunkte in kamera zeigen lassen\n",
    "        show_kpoints(main_frame, results)\n",
    "        \n",
    "        #k-points speichern in variable\n",
    "        kpoints = extract_keypoints(results)\n",
    "        num_frames.append(kpoints)\n",
    "        num_frames = num_frames[-30:]\n",
    "        \n",
    "        #wir führen die Vorhersage nur aus, wenn es 30 Frames sind\n",
    "        if len(num_frames) == 30:\n",
    "            res = model.predict(np.expand_dims(num_frames, axis = 0))[0]   #np expand dims makes sequence in required shape\n",
    "            print(gesten[np.argmax(res)])\n",
    "            \n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(satz) > 0: \n",
    "                    if gesten[np.argmax(res)] != satz[-1]:\n",
    "                        satz.append(gesten[np.argmax(res)])\n",
    "                else:\n",
    "                    satz.append(gesten[np.argmax(res)])\n",
    "            \n",
    "            #nur eine wort zeigen\n",
    "            if len(satz) > 1: \n",
    "                satz = satz[-1:]\n",
    "        \n",
    "        #platz für geste \"wort\"\n",
    "        cv.rectangle(main_frame, (0,0), (650, 50), (255, 255, 255), -1)\n",
    "        cv.putText(main_frame, ' '.join(satz), (240,45), cv.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 0), 3, cv.LINE_AA)\n",
    "        \n",
    "        cv.putText(main_frame, 'Projekt von Zaza Buzaladze', (370,475), \n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv.LINE_AA)\n",
    "            \n",
    "        cv.imshow(\"Kamera - Gestus\", main_frame)  #Anzeigen, Wiedergeben des Bildes\n",
    "        \n",
    "        if cv.waitKey(20) & 0xFF==ord(\"s\"):\n",
    "            break\n",
    "            #mit \"s\"(stop), stoppt kamera    \n",
    "\n",
    "    capture.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84855f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv fenster ausschalten (im absturz fall)\n",
    "capture.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a801dd42",
   "metadata": {},
   "source": [
    "# Quellen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4334c07a",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=doDUihpj6ro&list=PLyx6mI41rWldgZWHzHBGBQfP5MxFQNcFf&index=5&ab_channel=NicholasRenotte\n",
    "https://www.geeksforgeeks.org/python-list-files-in-a-directory/\n",
    "https://www.educba.com/keras-sequential/\n",
    "https://www.youtube.com/watch?v=8uC-WT1LYnU&ab_channel=Simplilearn\n",
    "https://www.educba.com/keras-dense/\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
    "https://towardsdatascience.com/softmax-activation-function-how-it-actually-works-d292d335bd78\n",
    "https://developers.google.com/mediapipe/solutions/vision/holistic_landmarker\n",
    "https://github.com/google/mediapipe/blob/master/docs/solutions/holistic.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f86581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
